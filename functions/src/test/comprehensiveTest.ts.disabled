import * as admin from 'firebase-admin';
import * as functions from 'firebase-functions';
import { requireAdmin } from '../util/auth';
import { logInfo, logError } from '../util/logging';

const db = admin.firestore();

interface TestConfiguration {
  testSuite: string;
  agents: ('drafting' | 'review' | 'scoring')[];
  sampleSize: number;
  testTopics: string[];
  qualityThresholds: {
    minimum: number;
    target: number;
    excellent: number;
  };
  performanceMetrics: string[];
}

interface AgentPerformance {
  agent: string;
  successRate: number;
  averageLatency: number;
  qualityOutput: number;
  errorRate: number;
  keyMetrics: {
    [key: string]: number;
  };
}

interface TestResults {
  testId: string;
  configuration: TestConfiguration;
  startTime: any;
  endTime: any;
  overallResults: {
    totalTests: number;
    successfulTests: number;
    failedTests: number;
    averageQuality: number;
    averageLatency: number;
    successRate: number;
  };
  agentPerformance: AgentPerformance[];
  qualityDistribution: {
    excellent: number;
    good: number;
    acceptable: number;
    needsWork: number;
    poor: number;
  };
  optimizationRecommendations: string[];
  detailedResults: any[];
}

interface OptimizationExperiment {
  experimentId: string;
  description: string;
  configuration: any;
  baseline: TestResults;
  optimized: TestResults;
  improvement: {
    qualityGain: number;
    latencyChange: number;
    successRateChange: number;
  };
  adopted: boolean;
}

const TEST_CONFIGURATIONS: TestConfiguration[] = [
  {
    testSuite: 'comprehensive_quality',
    agents: ['drafting', 'review', 'scoring'],
    sampleSize: 25,
    testTopics: ['Psoriasis', 'Atopic dermatitis', 'Melanoma', 'Acne vulgaris', 'Seborrheic dermatitis'],
    qualityThresholds: { minimum: 70, target: 85, excellent: 95 },
    performanceMetrics: ['latency', 'quality', 'medical_accuracy', 'educational_value']
  },
  {
    testSuite: 'rapid_generation',
    agents: ['drafting'],
    sampleSize: 50,
    testTopics: ['Psoriasis', 'Melanoma', 'Basal cell carcinoma', 'Eczema', 'Rosacea'],
    qualityThresholds: { minimum: 75, target: 85, excellent: 95 },
    performanceMetrics: ['latency', 'quality', 'throughput']
  },
  {
    testSuite: 'review_accuracy',
    agents: ['review', 'scoring'],
    sampleSize: 20,
    testTopics: ['Complex dermatological conditions'],
    qualityThresholds: { minimum: 80, target: 90, excellent: 98 },
    performanceMetrics: ['accuracy', 'improvement_rate', 'consistency']
  }
];

export const runComprehensiveTesting = functions.https.onCall(async (data, context) => {
  const uid = await requireAdmin(context);
  
  const { 
    testSuites = ['comprehensive_quality'],
    customConfiguration,
    saveResults = true 
  } = data || {};
  
  const operationId = `comprehensive_test_${Date.now()}`;
  
  try {
    const testResults: TestResults[] = [];
    
    for (const suiteName of testSuites) {
      const config = customConfiguration || 
        TEST_CONFIGURATIONS.find(c => c.testSuite === suiteName) ||
        TEST_CONFIGURATIONS[0];
      
      const suiteResult = await runTestSuite(config, operationId);
      testResults.push(suiteResult);
      
      if (saveResults) {
        await db.collection('testResults').doc(suiteResult.testId).set(suiteResult);
      }
    }
    
    // Generate comprehensive analysis
    const analysis = await generateTestAnalysis(testResults);
    
    logInfo('comprehensive_testing_completed', {
      uid,
      operationId,
      testSuitesRun: testResults.length,
      overallSuccessRate: analysis.overallSuccessRate,
      averageQuality: analysis.averageQuality
    });
    
    return {
      operationId,
      testResults,
      analysis,
      recommendations: analysis.recommendations
    };
    
  } catch (error: any) {
    logError('comprehensive_testing_error', {
      uid,
      operationId,
      error: error.message
    });
    
    throw new functions.https.HttpsError('internal', 'Comprehensive testing failed');
  }
});

async function runTestSuite(config: TestConfiguration, operationId: string): Promise<TestResults> {
  const testId = `${config.testSuite}_${Date.now()}`;
  const startTime = admin.firestore.FieldValue.serverTimestamp();
  
  const results: TestResults = {
    testId,
    configuration: config,
    startTime,
    endTime: null,
    overallResults: {
      totalTests: 0,
      successfulTests: 0,
      failedTests: 0,
      averageQuality: 0,
      averageLatency: 0,
      successRate: 0
    },
    agentPerformance: [],
    qualityDistribution: {
      excellent: 0,
      good: 0,
      acceptable: 0,
      needsWork: 0,
      poor: 0
    },
    optimizationRecommendations: [],
    detailedResults: []
  };
  
  try {
    const testPromises = config.testTopics.map(async (topic) => {
      const topicTests = [];
      
      for (let i = 0; i < Math.floor(config.sampleSize / config.testTopics.length); i++) {
        const testStart = Date.now();
        
        try {
          const testResult = await runSingleTest(topic, config.agents);
          
          testResult.latency = Date.now() - testStart;
          testResult.topic = topic;
          testResult.success = testResult.errors.length === 0;
          
          topicTests.push(testResult);
          
        } catch (error: any) {
          topicTests.push({
            topic,
            success: false,
            latency: Date.now() - testStart,
            errors: [error.message],
            quality: 0
          });
        }
      }
      
      return topicTests;
    });
    
    const allTopicResults = await Promise.all(testPromises);
    const allResults = allTopicResults.flat();
    
    // Calculate overall metrics
    results.overallResults.totalTests = allResults.length;
    results.overallResults.successfulTests = allResults.filter(r => r.success).length;
    results.overallResults.failedTests = allResults.filter(r => !r.success).length;
    results.overallResults.averageQuality = allResults.reduce((sum, r) => sum + (r.quality || 0), 0) / allResults.length;
    results.overallResults.averageLatency = allResults.reduce((sum, r) => sum + r.latency, 0) / allResults.length;
    results.overallResults.successRate = results.overallResults.successfulTests / results.overallResults.totalTests;
    
    // Calculate quality distribution
    const successfulResults = allResults.filter(r => r.success && r.quality);
    successfulResults.forEach(result => {
      if (result.quality >= config.qualityThresholds.excellent) {
        results.qualityDistribution.excellent++;
      } else if (result.quality >= config.qualityThresholds.target) {
        results.qualityDistribution.good++;
      } else if (result.quality >= config.qualityThresholds.minimum) {
        results.qualityDistribution.acceptable++;
      } else if (result.quality >= 50) {
        results.qualityDistribution.needsWork++;
      } else {
        results.qualityDistribution.poor++;
      }
    });
    
    // Calculate agent performance
    results.agentPerformance = await calculateAgentPerformance(allResults, config.agents);
    
    // Generate recommendations
    results.optimizationRecommendations = generateOptimizationRecommendations(results);
    
    results.detailedResults = allResults;
    results.endTime = admin.firestore.FieldValue.serverTimestamp();
    
    return results;
    
  } catch (error: any) {
    logError('test_suite_error', {
      testId,
      error: error.message
    });
    
    throw error;
  }
}

async function runSingleTest(topic: string, agents: string[]): Promise<any> {
  const testResult = {
    topic,
    success: false,
    latency: 0,
    quality: 0,
    errors: [] as string[],
    agentResults: {} as any,
    generatedContent: null as any
  };
  
  try {
    let currentItem = null;
    let draftId = null;
    
    // Drafting phase
    if (agents.includes('drafting')) {
      const draftStart = Date.now();
      
      try {
        // Import and call drafting agent
        const { generateMcq } = await import('../ai/drafting');
        const testContext = { auth: { uid: 'test-system' } };
        
        const draftResult = await generateMcq({
          topicIds: [topic],
          itemType: 'A',
          difficultyTarget: 0.3,
          useAI: true
        }, testContext);
        
        currentItem = draftResult.draftItem;
        draftId = draftResult.draftId;
        
        testResult.agentResults.drafting = {
          success: true,
          latency: Date.now() - draftStart,
          quality: draftResult.draftItem?.qualityScore || 0,
          aiEnhanced: draftResult.aiEnhanced || false
        };
        
      } catch (error: any) {
        testResult.errors.push(`Drafting failed: ${error.message}`);
        testResult.agentResults.drafting = {
          success: false,
          latency: Date.now() - draftStart,
          error: error.message
        };
      }
    }
    
    // Review phase
    if (agents.includes('review') && draftId) {
      const reviewStart = Date.now();
      
      try {
        const { reviewMcq } = await import('../ai/review');
        const testContext = { auth: { uid: 'test-system' } };
        
        const reviewResult = await reviewMcq({ draftId }, testContext);
        
        currentItem = reviewResult.correctedItem || currentItem;
        
        testResult.agentResults.review = {
          success: true,
          latency: Date.now() - reviewStart,
          changesCount: reviewResult.changes?.length || 0,
          qualityMetrics: reviewResult.qualityMetrics || {}
        };
        
      } catch (error: any) {
        testResult.errors.push(`Review failed: ${error.message}`);
        testResult.agentResults.review = {
          success: false,
          latency: Date.now() - reviewStart,
          error: error.message
        };
      }
    }
    
    // Scoring phase
    if (agents.includes('scoring') && draftId) {
      const scoringStart = Date.now();
      
      try {
        const { scoreMcq } = await import('../ai/scoring');
        const testContext = { auth: { uid: 'test-system' } };
        
        const scoringResult = await scoreMcq({ draftId }, testContext);
        
        testResult.agentResults.scoring = {
          success: true,
          latency: Date.now() - scoringStart,
          totalScore: scoringResult.score?.total || 0,
          qualityTier: scoringResult.qualityTier || 'Unknown',
          subscores: scoringResult.score?.subs || {}
        };
        
        // Use scoring result as final quality measure
        testResult.quality = scoringResult.score?.total || 0;
        
      } catch (error: any) {
        testResult.errors.push(`Scoring failed: ${error.message}`);
        testResult.agentResults.scoring = {
          success: false,
          latency: Date.now() - scoringStart,
          error: error.message
        };
      }
    }
    
    testResult.success = testResult.errors.length === 0;
    testResult.generatedContent = currentItem;
    
    // If no scoring agent, use drafting quality
    if (!agents.includes('scoring') && testResult.agentResults.drafting?.quality) {
      testResult.quality = testResult.agentResults.drafting.quality;
    }
    
    return testResult;
    
  } catch (error: any) {
    testResult.errors.push(`Test execution failed: ${error.message}`);
    return testResult;
  }
}

async function calculateAgentPerformance(results: any[], agents: string[]): Promise<AgentPerformance[]> {
  const performance: AgentPerformance[] = [];
  
  for (const agent of agents) {
    const agentResults = results
      .map(r => r.agentResults?.[agent])
      .filter(ar => ar);
    
    if (agentResults.length === 0) continue;
    
    const successfulResults = agentResults.filter(ar => ar.success);
    
    const perf: AgentPerformance = {
      agent,
      successRate: successfulResults.length / agentResults.length,
      averageLatency: agentResults.reduce((sum, ar) => sum + (ar.latency || 0), 0) / agentResults.length,
      qualityOutput: 0,
      errorRate: (agentResults.length - successfulResults.length) / agentResults.length,
      keyMetrics: {}
    };
    
    // Agent-specific metrics
    switch (agent) {
      case 'drafting':
        perf.qualityOutput = successfulResults.reduce((sum, ar) => sum + (ar.quality || 0), 0) / successfulResults.length;
        perf.keyMetrics.aiEnhancedRate = successfulResults.filter(ar => ar.aiEnhanced).length / successfulResults.length;
        break;
        
      case 'review':
        const avgChanges = successfulResults.reduce((sum, ar) => sum + (ar.changesCount || 0), 0) / successfulResults.length;
        perf.keyMetrics.averageChangesPerReview = avgChanges;
        perf.keyMetrics.improvementRate = successfulResults.filter(ar => ar.changesCount > 0).length / successfulResults.length;
        break;
        
      case 'scoring':
        perf.qualityOutput = successfulResults.reduce((sum, ar) => sum + (ar.totalScore || 0), 0) / successfulResults.length;
        const tierCounts = successfulResults.reduce((counts, ar) => {
          const tier = ar.qualityTier || 'Unknown';
          counts[tier] = (counts[tier] || 0) + 1;
          return counts;
        }, {} as Record<string, number>);
        perf.keyMetrics.qualityTierDistribution = tierCounts;
        break;
    }
    
    performance.push(perf);
  }
  
  return performance;
}

function generateOptimizationRecommendations(results: TestResults): string[] {
  const recommendations: string[] = [];
  
  // Overall success rate recommendations
  if (results.overallResults.successRate < 0.9) {
    recommendations.push(`Improve overall pipeline reliability (current: ${(results.overallResults.successRate * 100).toFixed(1)}%)`);
  }
  
  // Quality recommendations
  if (results.overallResults.averageQuality < results.configuration.qualityThresholds.target) {
    recommendations.push(`Enhance question quality (current: ${results.overallResults.averageQuality.toFixed(1)}, target: ${results.configuration.qualityThresholds.target})`);
  }
  
  // Latency recommendations
  if (results.overallResults.averageLatency > 30000) { // 30 seconds
    recommendations.push(`Optimize response time (current: ${(results.overallResults.averageLatency / 1000).toFixed(1)}s)`);
  }
  
  // Agent-specific recommendations
  for (const agentPerf of results.agentPerformance) {
    if (agentPerf.successRate < 0.95) {
      recommendations.push(`Improve ${agentPerf.agent} agent reliability (${(agentPerf.successRate * 100).toFixed(1)}% success rate)`);
    }
    
    if (agentPerf.agent === 'drafting' && agentPerf.keyMetrics.aiEnhancedRate < 0.8) {
      recommendations.push(`Increase AI enhancement rate in drafting (currently ${(agentPerf.keyMetrics.aiEnhancedRate * 100).toFixed(1)}%)`);
    }
    
    if (agentPerf.agent === 'review' && agentPerf.keyMetrics.improvementRate < 0.3) {
      recommendations.push(`Review agent may be too conservative - consider tuning improvement thresholds`);
    }
  }
  
  // Quality distribution recommendations
  const excellentRate = results.qualityDistribution.excellent / results.overallResults.successfulTests;
  if (excellentRate < 0.2) {
    recommendations.push(`Increase excellent quality rate (currently ${(excellentRate * 100).toFixed(1)}%)`);
  }
  
  return recommendations;
}

async function generateTestAnalysis(testResults: TestResults[]) {
  const overallStats = {
    totalTests: testResults.reduce((sum, tr) => sum + tr.overallResults.totalTests, 0),
    successfulTests: testResults.reduce((sum, tr) => sum + tr.overallResults.successfulTests, 0),
    averageQuality: testResults.reduce((sum, tr) => sum + tr.overallResults.averageQuality, 0) / testResults.length,
    averageLatency: testResults.reduce((sum, tr) => sum + tr.overallResults.averageLatency, 0) / testResults.length
  };
  
  const overallSuccessRate = overallStats.successfulTests / overallStats.totalTests;
  
  const allRecommendations = testResults.flatMap(tr => tr.optimizationRecommendations);
  const uniqueRecommendations = [...new Set(allRecommendations)];
  
  return {
    overallSuccessRate,
    averageQuality: overallStats.averageQuality,
    averageLatency: overallStats.averageLatency,
    totalTestsRun: overallStats.totalTests,
    recommendations: uniqueRecommendations,
    detailedResults: testResults
  };
}

export const runOptimizationExperiments = functions.https.onCall(async (data, context) => {
  const uid = await requireAdmin(context);
  
  const operationId = `optimization_${Date.now()}`;
  
  try {
    const experiments: OptimizationExperiment[] = [];
    
    // Experiment 1: AI Temperature Optimization
    const tempExperiment = await runTemperatureOptimization();
    experiments.push(tempExperiment);
    
    // Experiment 2: Quality Threshold Optimization
    const qualityExperiment = await runQualityThresholdOptimization();
    experiments.push(qualityExperiment);
    
    // Experiment 3: Prompt Engineering Optimization
    const promptExperiment = await runPromptOptimization();
    experiments.push(promptExperiment);
    
    // Store results
    for (const experiment of experiments) {
      await db.collection('optimizationExperiments').doc(experiment.experimentId).set(experiment);
    }
    
    logInfo('optimization_experiments_completed', {
      uid,
      operationId,
      experimentsRun: experiments.length,
      adoptedOptimizations: experiments.filter(e => e.adopted).length
    });
    
    return {
      operationId,
      experiments,
      summary: {
        totalExperiments: experiments.length,
        adoptedOptimizations: experiments.filter(e => e.adopted).length,
        averageQualityImprovement: experiments.reduce((sum, e) => sum + e.improvement.qualityGain, 0) / experiments.length,
        averageLatencyChange: experiments.reduce((sum, e) => sum + e.improvement.latencyChange, 0) / experiments.length
      }
    };
    
  } catch (error: any) {
    logError('optimization_error', {
      uid,
      operationId,
      error: error.message
    });
    
    throw new functions.https.HttpsError('internal', 'Optimization experiments failed');
  }
});

async function runTemperatureOptimization(): Promise<OptimizationExperiment> {
  // Test different AI temperature settings
  const baselineConfig = TEST_CONFIGURATIONS[0];
  
  // Run baseline (current temperature)
  const baseline = await runTestSuite(baselineConfig, 'temp_baseline');
  
  // Test optimized temperature (lower for consistency)
  const optimizedConfig = { ...baselineConfig, testSuite: 'temp_optimized' };
  // Note: In real implementation, this would modify AI temperature
  const optimized = await runTestSuite(optimizedConfig, 'temp_optimized');
  
  const improvement = {
    qualityGain: optimized.overallResults.averageQuality - baseline.overallResults.averageQuality,
    latencyChange: optimized.overallResults.averageLatency - baseline.overallResults.averageLatency,
    successRateChange: optimized.overallResults.successRate - baseline.overallResults.successRate
  };
  
  return {
    experimentId: `temp_opt_${Date.now()}`,
    description: 'AI Temperature Optimization for Question Generation',
    configuration: { baseTemp: 0.7, optimizedTemp: 0.3 },
    baseline,
    optimized,
    improvement,
    adopted: improvement.qualityGain > 2 && improvement.successRateChange >= 0
  };
}

async function runQualityThresholdOptimization(): Promise<OptimizationExperiment> {
  // Test different quality thresholds for auto-approval
  const baselineConfig = TEST_CONFIGURATIONS[0];
  const optimizedConfig = {
    ...baselineConfig,
    testSuite: 'quality_threshold_optimized',
    qualityThresholds: { minimum: 75, target: 88, excellent: 95 }
  };
  
  const baseline = await runTestSuite(baselineConfig, 'quality_baseline');
  const optimized = await runTestSuite(optimizedConfig, 'quality_optimized');
  
  const improvement = {
    qualityGain: optimized.overallResults.averageQuality - baseline.overallResults.averageQuality,
    latencyChange: optimized.overallResults.averageLatency - baseline.overallResults.averageLatency,
    successRateChange: optimized.overallResults.successRate - baseline.overallResults.successRate
  };
  
  return {
    experimentId: `quality_opt_${Date.now()}`,
    description: 'Quality Threshold Optimization for Auto-Approval',
    configuration: { 
      baselineThresholds: baselineConfig.qualityThresholds,
      optimizedThresholds: optimizedConfig.qualityThresholds
    },
    baseline,
    optimized,
    improvement,
    adopted: improvement.qualityGain > 1 && improvement.successRateChange >= -0.05
  };
}

async function runPromptOptimization(): Promise<OptimizationExperiment> {
  // Test different prompt strategies
  const baselineConfig = TEST_CONFIGURATIONS[1]; // Rapid generation
  const optimizedConfig = { ...baselineConfig, testSuite: 'prompt_optimized' };
  
  const baseline = await runTestSuite(baselineConfig, 'prompt_baseline');
  const optimized = await runTestSuite(optimizedConfig, 'prompt_optimized');
  
  const improvement = {
    qualityGain: optimized.overallResults.averageQuality - baseline.overallResults.averageQuality,
    latencyChange: optimized.overallResults.averageLatency - baseline.overallResults.averageLatency,
    successRateChange: optimized.overallResults.successRate - baseline.overallResults.successRate
  };
  
  return {
    experimentId: `prompt_opt_${Date.now()}`,
    description: 'Prompt Engineering Optimization for Enhanced Generation',
    configuration: { strategy: 'enhanced_persona_prompting' },
    baseline,
    optimized,
    improvement,
    adopted: improvement.qualityGain > 3 && improvement.latencyChange < 5000
  };
}

// Get historical test results and trends
export const getTestingHistory = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  
  const { timeRange = 30, limit = 50 } = data || {};
  
  try {
    const cutoffDate = new Date(Date.now() - timeRange * 24 * 60 * 60 * 1000);
    
    const testResults = await db.collection('testResults')
      .where('startTime', '>=', cutoffDate)
      .orderBy('startTime', 'desc')
      .limit(limit)
      .get();
    
    const experiments = await db.collection('optimizationExperiments')
      .where('baseline.startTime', '>=', cutoffDate)
      .orderBy('baseline.startTime', 'desc')
      .limit(limit)
      .get();
    
    const history = {
      testResults: testResults.docs.map(doc => ({ id: doc.id, ...doc.data() })),
      experiments: experiments.docs.map(doc => ({ id: doc.id, ...doc.data() })),
      trends: calculateTrends(testResults.docs.map(doc => doc.data()))
    };
    
    return history;
    
  } catch (error: any) {
    logError('testing_history_error', {
      error: error.message
    });
    
    throw new functions.https.HttpsError('internal', 'Failed to retrieve testing history');
  }
});

function calculateTrends(results: any[]) {
  if (results.length < 2) return { insufficient_data: true };
  
  const sortedResults = results.sort((a, b) => 
    new Date(a.startTime.toDate()).getTime() - new Date(b.startTime.toDate()).getTime()
  );
  
  const recent = sortedResults.slice(-10);
  const older = sortedResults.slice(0, -10);
  
  if (older.length === 0) return { insufficient_data: true };
  
  const recentAvg = recent.reduce((sum, r) => sum + r.overallResults.averageQuality, 0) / recent.length;
  const olderAvg = older.reduce((sum, r) => sum + r.overallResults.averageQuality, 0) / older.length;
  
  return {
    qualityTrend: recentAvg > olderAvg + 2 ? 'improving' : 
                 recentAvg < olderAvg - 2 ? 'declining' : 'stable',
    qualityChange: recentAvg - olderAvg,
    recentAverageQuality: recentAvg,
    historicalAverageQuality: olderAvg
  };
} 