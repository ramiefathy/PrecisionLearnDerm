/**
 * Streaming Gemini API Client
 * Implements streaming responses to provide visibility into AI thinking process
 * and better handle long-running generation tasks
 */

import { GoogleGenAI, HarmCategory, HarmBlockThreshold } from '@google/genai';
import { getGeminiApiKey } from './config';
import { logInfo, logError } from './logging';
import * as admin from 'firebase-admin';

export interface StreamingGeminiOptions {
  maxRetries?: number;
  baseDelay?: number;
  maxDelay?: number;
  timeout?: number;
  fallbackToFlash?: boolean;
  onChunk?: (text: string) => void; // Callback for streaming chunks
}

export interface StreamingGeminiRequest {
  prompt: string;
  operation: string;
  preferredModel?: 'gemini-2.0-flash-exp' | 'gemini-1.5-flash';
  temperature?: number;
  maxTokens?: number;
  onChunk?: (text: string) => void;
}

export interface StreamingGeminiResult {
  success: boolean;
  text?: string;
  chunks?: string[];
  error?: string;
  model?: string;
  attempts?: number;
  duration?: number;
}

/**
 * Streaming Gemini API client for real-time response visibility
 */
export class StreamingGeminiClient {
  private genAI: GoogleGenAI;
  private options: Required<StreamingGeminiOptions>;
  private sessionId?: string;
  private db = admin.firestore();

  constructor(options: StreamingGeminiOptions = {}, sessionId?: string) {
    const apiKey = getGeminiApiKey();
    this.genAI = new GoogleGenAI({ apiKey });
    this.sessionId = sessionId;
    
    this.options = {
      maxRetries: options.maxRetries || 2,
      baseDelay: options.baseDelay || 1000,
      maxDelay: options.maxDelay || 10000,
      timeout: options.timeout || 120000, // 2 minutes default
      fallbackToFlash: options.fallbackToFlash !== false,
      onChunk: options.onChunk || (() => {})
    };
  }

  setSessionId(sessionId: string) {
    this.sessionId = sessionId;
  }

  private async updateProgress(stage: string, status: string, details?: any) {
    if (!this.sessionId) return;
    
    try {
      const progressRef = this.db.collection('generationProgress').doc(this.sessionId);
      const doc = await progressRef.get();
      
      if (doc.exists) {
        await progressRef.update({
          [`stages.${stage}.status`]: status,
          [`stages.${stage}.lastUpdate`]: new Date().toISOString(),
          [`stages.${stage}.details`]: details || {},
          lastUpdate: new Date().toISOString()
        });
      }
    } catch (error) {
      logError('Failed to update progress', { error, sessionId: this.sessionId });
    }
  }

  private async updateProgressChunk(chunk: string) {
    if (!this.sessionId) return;
    
    try {
      const progressRef = this.db.collection('generationProgress').doc(this.sessionId);
      const doc = await progressRef.get();
      
      if (doc.exists) {
        await progressRef.update({
          'currentChunk': chunk,
          'lastChunkTime': new Date().toISOString(),
          'chunks': admin.firestore.FieldValue.arrayUnion({
            timestamp: new Date().toISOString(),
            content: chunk.substring(0, 100), // Store preview only
            length: chunk.length
          })
        });
      }
    } catch (error) {
      logError('Failed to update progress chunk', { error, sessionId: this.sessionId });
    }
  }

  /**
   * Generate text with streaming for real-time visibility
   */
  async generateTextStream(request: StreamingGeminiRequest): Promise<StreamingGeminiResult> {
    const startTime = Date.now();
    const { prompt, operation, preferredModel = 'gemini-2.0-flash-exp', onChunk } = request;
    const chunkCallback = onChunk || this.options.onChunk;
    
    logInfo('streaming_gemini_request_started', {
      operation,
      preferredModel,
      promptLength: prompt.length
    });

    // Try preferred model first
    let result = await this.tryModelWithStream(prompt, preferredModel, operation, chunkCallback);
    
    // Fallback if needed
    if (!result.success && this.options.fallbackToFlash && preferredModel === 'gemini-2.0-flash-exp') {
      logInfo('streaming_gemini_fallback_to_flash', {
        operation,
        originalError: result.error
      });
      
      result = await this.tryModelWithStream(prompt, 'gemini-1.5-flash', operation, chunkCallback);
    }

    const duration = Date.now() - startTime;
    result.duration = duration;

    if (result.success) {
      logInfo('streaming_gemini_success', {
        operation,
        model: result.model,
        attempts: result.attempts,
        duration,
        textLength: result.text?.length || 0,
        chunkCount: result.chunks?.length || 0
      });
    } else {
      logError('streaming_gemini_final_failure', {
        operation,
        error: result.error,
        attempts: result.attempts,
        duration
      });
    }

    return result;
  }

  /**
   * Try streaming with a specific model
   */
  private async tryModelWithStream(
    prompt: string,
    modelName: 'gemini-2.0-flash-exp' | 'gemini-1.5-flash',
    operation: string,
    onChunk: (text: string) => void
  ): Promise<StreamingGeminiResult> {
    for (let attempt = 1; attempt <= this.options.maxRetries; attempt++) {
      try {
        logInfo('streaming_gemini_attempt', {
          operation,
          model: modelName,
          attempt
        });

        const result = await this.makeStreamingRequest(prompt, modelName, operation, onChunk);
        
        if (result.success) {
          return {
            ...result,
            model: modelName,
            attempts: attempt
          };
        }
        
        // Wait before retrying
        if (attempt < this.options.maxRetries) {
          const delay = this.calculateDelay(attempt);
          await this.delay(delay);
        }

      } catch (error: any) {
        logError('streaming_gemini_attempt_error', {
          operation,
          model: modelName,
          attempt,
          error: error.message || String(error)
        });
        
        if (attempt < this.options.maxRetries) {
          const delay = this.calculateDelay(attempt);
          await this.delay(delay);
        }
      }
    }

    return {
      success: false,
      error: 'All attempts failed',
      model: modelName,
      attempts: this.options.maxRetries
    };
  }

  /**
   * Make a single streaming request to the Gemini API
   */
  private async makeStreamingRequest(
    prompt: string,
    modelName: string,
    operation: string,
    onChunk: (text: string) => void
  ): Promise<StreamingGeminiResult> {
    return new Promise(async (resolve) => {
      const timeoutId = setTimeout(() => {
        logError('streaming_gemini_timeout', {
          operation,
          model: modelName,
          timeout: this.options.timeout
        });
        resolve({
          success: false,
          error: `Stream timed out after ${this.options.timeout}ms`
        });
      }, this.options.timeout);

      try {
        const generationConfig = {
          temperature: 0.7,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 8192
        };
        
        const safetySettings = [
          {
            category: HarmCategory.HARM_CATEGORY_HARASSMENT,
            threshold: HarmBlockThreshold.OFF
          },
          {
            category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold: HarmBlockThreshold.OFF
          },
          {
            category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
            threshold: HarmBlockThreshold.OFF
          },
          {
            category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
            threshold: HarmBlockThreshold.OFF
          }
        ];

        // For new @google/genai API, we configure everything in constructor
        const config = {
          model: modelName,
          generationConfig,
          safetySettings
        };
        const model = this.genAI.getGenerativeModel(config);

        logInfo('streaming_request_started', {
          operation,
          model: modelName,
          promptLength: prompt.length
        });

        // Update progress before starting
        await this.updateProgress('drafting', 'running', {
          model: modelName,
          promptLength: prompt.length
        });

        // Use streaming API
        const result = await model.generateContentStream(prompt);
        
        const chunks: string[] = [];
        let fullText = '';
        
        // Process the stream
        for await (const chunk of result.stream) {
          const chunkText = chunk.text();
          if (chunkText) {
            chunks.push(chunkText);
            fullText += chunkText;
            
            // Update progress with chunk for Firebase tracking
            await this.updateProgressChunk(chunkText);
            
            // Call the chunk callback for real-time visibility
            onChunk(chunkText);
            
            // Log progress
            if (chunks.length % 10 === 0) {
              logInfo('streaming_progress', {
                operation,
                model: modelName,
                chunksReceived: chunks.length,
                totalLength: fullText.length
              });
            }
          }
        }
        
        // Update progress when complete
        await this.updateProgress('drafting', 'complete', {
          responseLength: fullText.length,
          chunkCount: chunks.length
        });
        
        clearTimeout(timeoutId);
        
        logInfo('streaming_complete', {
          operation,
          model: modelName,
          totalChunks: chunks.length,
          finalLength: fullText.length
        });

        if (fullText) {
          resolve({
            success: true,
            text: fullText,
            chunks
          });
        } else {
          resolve({
            success: false,
            error: 'No content generated from stream'
          });
        }

      } catch (error: any) {
        clearTimeout(timeoutId);
        
        const errorMessage = this.extractErrorMessage(error);
        logError('streaming_gemini_api_error', {
          operation,
          model: modelName,
          error: errorMessage
        });
        
        resolve({
          success: false,
          error: errorMessage
        });
      }
    });
  }

  /**
   * Extract meaningful error message from various error types
   */
  private extractErrorMessage(error: any): string {
    if (error.message) {
      if (error.message.includes('[500 Internal Server Error]')) {
        return 'Gemini API server error (500) - service temporarily unavailable';
      }
      if (error.message.includes('[429 Too Many Requests]')) {
        return 'Gemini API rate limit exceeded';
      }
      return error.message;
    }
    return 'Unknown Gemini API error';
  }

  /**
   * Calculate delay with exponential backoff
   */
  private calculateDelay(attempt: number): number {
    const exponentialDelay = this.options.baseDelay * Math.pow(2, attempt - 1);
    const jitter = Math.random() * 0.1 * exponentialDelay;
    return Math.min(exponentialDelay + jitter, this.options.maxDelay);
  }

  /**
   * Utility delay function
   */
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

/**
 * Global instance of the streaming Gemini client
 */
let globalStreamingClient: StreamingGeminiClient | null = null;

/**
 * Get or create the global streaming Gemini client instance
 */
export function getStreamingGeminiClient(options?: StreamingGeminiOptions): StreamingGeminiClient {
  if (!globalStreamingClient) {
    globalStreamingClient = new StreamingGeminiClient(options);
  }
  return globalStreamingClient;
}

/**
 * Convenience function for streaming text generation
 */
export async function generateTextWithStream(
  prompt: string,
  operation: string,
  onChunk?: (text: string) => void,
  preferredModel: 'gemini-2.0-flash-exp' | 'gemini-1.5-flash' = 'gemini-2.0-flash-exp'
): Promise<string> {
  const client = getStreamingGeminiClient();
  const result = await client.generateTextStream({
    prompt,
    operation,
    preferredModel,
    onChunk
  });
  
  if (result.success && result.text) {
    return result.text;
  }
  
  throw new Error(result.error || 'Failed to generate text');
}